{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facd88e6",
   "metadata": {},
   "source": [
    "# Working with different file formats \n",
    "\n",
    "### Objective:\n",
    "1. Data Engineering\n",
    "2. Data Engineering Process\n",
    "3. Working with different file formats\n",
    "4. Data Analysis\n",
    "\n",
    "\n",
    "### 1- Data Engineering\n",
    "**Data Engineering:** is s one of the most critical and foundational skills in any data scientistâ€™s toolkit.\n",
    "\n",
    "### 2- Data Engineering Process:\n",
    "\n",
    "There are several steps in Data Engineering process:\n",
    "*   Extract - Data extraction is getting data from multiple sources. Ex. Data extraction from a website using Web scraping or gathering information from the data that are stored in different formats (JSON, CSV, XLSX etc.).\n",
    "\n",
    "* Transform - Transforming the data means removing the data that we don't need for further analysis and converting the data in the format that all the data from the multiple sources is in the same format.\n",
    "\n",
    "* Load - Loading the data inside a data warehouse. Data warehouse essentially contains large volumes of data that are accessed to gather insights.\n",
    "\n",
    "### 3- Working with different file formats: \n",
    "In the real-world, people rarely get neat tabular data. Thus, it is mandatory for any data scientist\n",
    "(or data engineer) to be aware of different file formats, common challenges in handling them and the best, most efficient ways to handle this data in real life. \n",
    "\n",
    "**A file format** is a standardized method for encoding information to be stored in a computer file.\n",
    "\n",
    "1.  It specifies whether the file is binary or ASCII (text-based).\n",
    "\n",
    "2. It dictates how the information is organized within the file (e.g., the Comma-Separated Values, or CSV format, organizes tabular data as plain text).\n",
    "\n",
    "3. The file format can typically be identified by its file extension (e.g., a file named Data.csv indicates the CSV format).\n",
    "\n",
    "#### how to load a dataset into our Jupyter Notebook.\n",
    "\n",
    "1. **Comma-separated values (CSV) file format**\n",
    "\n",
    "* The Comma-separated values file format falls under a spreadsheet file format.\n",
    "\n",
    "* In a spreadsheet file format, data is stored in cells.  Each cell is organized in rows and columns.\n",
    "\n",
    "* A column in the spreadsheet file can have different types. For example, a column can be of string type, a date type, or an integer type.\n",
    "\n",
    "* Each line in CSV file represents an observation, or commonly called a record. Each record may contain one or more fields which are separated by a comma.\n",
    "\n",
    "**Reading data from CSV in Python**\n",
    "\n",
    "Will use pandas to read CSV files\n",
    "\n",
    "**pandas.read_csv()** function to read the csv file. In the parentheses, we put the file path along with a quotation mark as an argument, so that pandas will read the file into a data frame from that address. The file path can be either a URL or your local file address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c768da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Website version \n",
    "#import piplite\n",
    "#await piplite.install(['seaborn', 'lxml', 'openpyxl'])\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "# Laptop, in the powershell or cmd, or Bash\n",
    "#pip install seaborn lxml openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863deb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# for Website Version just uncomment the code:\n",
    "#from pyodide.http import pyfetch\n",
    "\n",
    "#filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n",
    "\n",
    "#async def download(url, filename):\n",
    "    #response = await pyfetch(url)\n",
    "    #if response.status == 200:\n",
    "     #   with open(filename, \"wb\") as f:\n",
    "      #      f.write(await response.bytes())\n",
    "\n",
    "#await download(filename, \"addresses.csv\")\n",
    "\n",
    "#df = pd.read_csv(\"addresses.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16518f34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For VS version\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os # Used for basic file system operations\n",
    "\n",
    "# --- Configuration ---\n",
    "# The URL of the file to download\n",
    "file_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n",
    "\n",
    "# The name we want to save the file as locally\n",
    "LOCAL_FILENAME = \"addresses.csv\"\n",
    "local_path = \"A:\\Coursera\\1-computer-science\\python-and-git-practice\\ibm-python-for-data-sc-ai-Dev\\module5-API-and-data-collection\"\n",
    "def download_file_synchronously(url, local_path):\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL using the synchronous 'requests' library.\n",
    "    This replaces the async 'pyfetch' mechanism used in pyodide environments.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the URL. We use stream=True for potentially large files.\n",
    "        print(f\"Starting download of {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        # Raise an HTTPError if the status code is 4xx or 5xx\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Write the content to the local file\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            # Write content in chunks (8KB at a time) to save memory\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"Download successful. File saved as: {LOCAL_FILENAME}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during download: {e}\")\n",
    "        # Exit the script if the download fails\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "# 1. Execute the synchronous download\n",
    "download_file_synchronously( file_url, LOCAL_FILENAME)\n",
    "\n",
    "# 2. Read the downloaded file into a Pandas DataFrame\n",
    "# Note: The file is now guaranteed to exist locally, just like in the original code.\n",
    "try:\n",
    "    df = pd.read_csv(LOCAL_FILENAME, header=None)\n",
    "    \n",
    "    print(\"\\n--- DataFrame Loaded Successfully ---\")\n",
    "    print(\"First 5 rows of the DataFrame:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Optional: Clean up the downloaded file after use\n",
    "    # os.remove(LOCAL_FILENAME)\n",
    "    # print(f\"\\nCleaned up local file: {LOCAL_FILENAME}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the file {LOCAL_FILENAME}. Check download status.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The CSV file is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file reading: {e}\")\n",
    "\n",
    "# Adding column name to the DataFrame\n",
    "df.columns =['First Name', 'Last Name', 'Location ', 'City','State','Area Code']\n",
    "\n",
    "# Selecting a single column\n",
    "print(df[\"First Name\"])\n",
    "\n",
    "#Selecting multiple columns\n",
    "df = df[['First Name', 'Last Name', 'Location ', 'City','State','Area Code']]\n",
    "print(df)\n",
    "# Selecting rows using .iloc and .loc\n",
    "# loc() : loc() is label based data selecting method which means that we have to pass the name of \n",
    "# the row or column which we want to select.\n",
    "# To select the first row\n",
    "df.loc[0] \n",
    "print(df)\n",
    "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
    "df.loc[[0,1,2], \"First Name\" ]\n",
    "print(df)\n",
    "\n",
    "# iloc() : iloc() is a indexed based selecting method which means that we have to pass integer\n",
    "#  index in the method to select specific row/column.\n",
    "\n",
    "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
    "df.iloc[[0,1,2], 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b467d1f",
   "metadata": {},
   "source": [
    "2* **Transform Function in Pandas**\n",
    "\n",
    "Python's Transform function returns a self-produced dataframe with transformed values after applying the function specified in its parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ac21a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = df.transform(func = lambda x : x + 10)\n",
    "print(df)\n",
    "\n",
    "#applying the transform function\n",
    "df = df.transform(func = lambda x : x + 10)\n",
    "print(df)\n",
    "\n",
    "# Now we will use DataFrame.transform() function to find the square root to each element of the dataframe.\n",
    "result = df.transform(func = ['sqrt'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
